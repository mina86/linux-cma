\chapter{Wstęp}

Nowoczesne procesory przeznaczone do serwerów i~komputerów osobistych
posiadają jednostkę zarządzania pamięcią (\ang{memory management
  unit}, MMU), która tłumaczy adresy logiczne, widziane przez
procesor, na adresy fizyczne.  Dzięki temu, obszary pamięci, które
z~punktu widzenia procesora (a~więc programów na nim uruchomionych)
mają ciągłe adresy, mogą w~rzeczywistości być podzielone na wiele
stron rozrzuconych po pamięci fizycznej.  W~ten sposób, nawet jeżeli
program alokuje wielomegabajtowy\footnote{Aby uniknąć wieloznaczności,
  stosuję przedrostki „kilo-” i~„mega-” w~znaczeniu odpowiednio tysiąc
  i~milion (zgodnie z~układem SI), a~„kibi-” i~„mebi-” w~znaczeniu
  odpowiednie $2^{10} = 1024$ i~$2^{20}$ (zgodnie
  z~tzw.\ przedrostkami ICE).} obszary, system może zrealizować
żądanie alokując wiele czterokilobajtowych stron i~nie przejmować się
fragmentacją pamięci.

Niemniej, MMU nie jest zazwyczaj dostępny dla pozostałych układów
zintegrowanych znajdujących się w~urządzeniu, takich jak np.\ karta
dźwiękowa, czy kontroler sieciowy.  Jednak także i~dla tych przypadków
istnieją rozwiązania w~postaci mechanizmu bezpośredniego dostępu do
pamięci (\ang{Dircet Memory Access}, DMA) z~obsługą mechanizmu
vectored IO \TODO{przetłumaczyć}, który pozwala na „złożenie” buforu
z~wielu mniejszych buforów (stąd też inna nazwa tego mechanizmu:
\ang{scatter-gather}).

Niestety, zarówno MMU jak i~DMA obsługujące vectored IO
\TODO{przetłumaczyć} nie są pozbawione kosztów -- zarówno związanych
z~koniecznością fizycznego wyprodukowaniom odpowiedniego układu, ale
także związanych z~szybkością dostępu do pamięci.  Z~tego powodu,
w~systemach wbudowanych, takich jak np.\ telefony komórkowe, takie
mechanizmy są często niedostępne dla zewnętrznych (z~punktu widzenia
procesora) układów.  Jednocześnie, właśnie takie urządzenia posiadają
dużo takich układów, jak chociażby aparat fotograficzny, czy układ do
szybkiego dekodowania strumieni multimedialnych.

Powoduje to, że tego typu układy muszą operować bezpośrednio na
adresach fizycznych i~w~konsekwencji, wszelkie stosowane przez nie
bufory muszą być ciągłe w~pamięci fizycznej.  Niestety, Linux nie jest
dobrze przystosowany do alokowania dużych obszarów ciągłych
fizycznie\TODO{zbyt dużo „fizycznie”}\footnote{Dla przykładu,
  pięciomegapikselowa kamera potrzebuje buforu o~rozmiarze 15
  megabajtów, a~pojedyncza ramka {\it full HD} (tj.\ $1920 \times
  1080$) zajmuje ponad sześć megabajtów (i~dodatkowo dekoder musi
  często przechowywać kilka ramek).  Linux nie jest nawet w~stanie
  (bez modyfikowania źródła) zarządzać obszarami większymi niż cztery
  megabajty (1024 strony).  Na domiar złego fragmentacja zewnętrzna
  utrudnia alokacje, a~fragmentacja wewnętrzna powoduje duże straty
  pamięci.}.

Najprostszym, i~stosunkowo często stosowanym, rozwiązaniem tego
problemu jest zarezerwowanie przy starcie systemu pewnego obszaru
pamięci na potrzeby danego sterownika.  Niestety, o~ile taki mechanizm
może być wystarczający, jeżeli podzespoły wymagają stosunkowo małych
buforów, przestaje się on skalować przy współczesnych systemach, gdyż
wymaga rezerwacji wielu megabajtów pamięci, która przez większość
czasu nie jest do niczego wykorzystywana.

\section{Physical Memory Manager}\label{sec:pmm}

Bardziej skomplikowanym rozwiązaniem jest mechanizm, który rezerwuje
pewną przestrzeń pamięci, ale zamiast na stałe przypisywać regiony
pamięci do urządzeń, pozwala sterownikom alokować bufory, wtedy, gdy
są one potrzebne.

W~trakcie moich prac napisałem {\it Physical Memory Manager} (lub PMM)
\cite{patch:pmm}, który implementuje dokładnie te założenia. W~tym
podstawowym założeniu, PMM nie przedstawia sobą nic nowego.  Bowiem
w~1996 roku Matt Welsh napisał pierwszą wersję patcha
\emph{bigphysarea} dla jądra 1.3.71, który był z~różnym zaangażowaniem
utrzymywany i~przystosowywany do nowszych wersji Linuksa
\cite{patch:bigphys}.

PMM umożliwiał alokowanie dużych obszarów ciągłej pamięci fizycznej
nie tylko sterownikom działającym w~przestrzeni jądra, ale także
programom działających pod kontrolą systemu.  W~ten sposób, aplikacja
mogła zaalokować pamięć, tak aby (dla przykładu) dekoder JPEG mógł
zdekodować plik graficzny i~umieścić go w~pamięci bezpośrednio
dostępnej dla aplikacji.  W~ten sposób, kod odpowiedzialny za
mapowanie zaalokowanej pamięci był umieszczony wewnątrz PMM in
poszczególne sterowniki nie musiały się tym problemem zajmować.

Co więcej, PMM został zintegrowany z~mechanizmem współdzielenia
pamięci Systemu V~(tj.\ funkcjami \lstinline|shmget()|,
\lstinline|shmat()|, \lstinline|shmdt()| itp).  Mechanizm ten jest
wykorzystywany między innymi przez system X~Window do umożliwienia
współdzielenia pixmap pomiędzy klientem i~serwerem (działającymi na
tej samej maszynie) bez konieczności przysłania danych przez gniazdo
sieciowe.

Pozwalało to na dekodowanie obrazów i~strumieni video bezpośrednio do
obszarów pamięci z~których X11 odczytywał dane.  Dzięki temu, w~całym
procesie dane nie były niepotrzebnie kopiowane z~jednego miejsca
w~pamięci do drugiego, co minimalizowało zużycie procesora i~szyny
pamięci i~w~rezultacie przyśpieszało działanie systemu.

Jednakże, pamięć zarezerwowana przez PMM i~tak przez większość czasu
jest zupełnie nieużywana, a~więc marnowana.  Z~tego powodu, PMM
w~formie zaprezentowanej na początku nie został przyjęty przez
społeczność programistów Linuksa i~musiałem rozwijać go dalej.

\section{Contiguous Memory Allocator}\label{sec:cma}

Aby rozwiązać i~ten problem, zacząłem rozwijać {\it Contiguous Memory
  Allocator} (lub CMA).  W~swoich początkowych wersjach, i~ten
mechanizm działał na założeniach identycznych do PMM -- rezerwował
pamięć przy starcie systemu, którą potem zarządzał pozwalając
sterownikom i~programom na alokowanie obszarów ciągłych fizycznie
\cite{patch:cma-4}.

Pierwsze wersje CMA skupiały się głównie na rozwiązywaniu problemu
przypisywania różnych zarezerwowanych obszarów do różnych urządzeń,
a~także umożliwianiu sterownikom alokowanie różnych buforów w~różnych
obszarach pamięci.  Było to potrzebne, gdyż dekoder wideo
(\ang{Multi-Format Codec}, lub MFC) stosowany na platformie S5PV110
wymagał, aby różne dane były dostępne w~różnych bankach pamięci.
Pozwalało to na zwiększenie szybkości dostępu do tych danych dzięki
zastosowaniu odczytu z~dwóch banków pamięci jednocześnie.

Z~czasem, coraz bardziej integrowałem CMA z~kodem zarządzania pamięci
w~Linuksie w~wyniku czego, pamięć rezerwowana przy starcie systemu,
stała się dostępna dla reszty systemu, jeżeli żaden sterownik jej nie
używał \cite{patch:cma-24}.  Takie rozwiązanie zostało ostatecznie
zaakceptowane przez społeczność deweloperów Linuksa i~jest dostępne
w~źródłach Linuksa począwszy od wersji 3.5.  W~tej pracy opisuję CMA
w~formie w~jakiej znalazł się on w~Linuksie 3.5\footnote{Należy
  zauważyć, że Linuks jest szybko rozwijającym się projektem wolnego
  oprogramowania i~ponieważ CMA używana jest przez coraz więcej osób,
  jest ona ciągle rozwijana i~im dalej w~przyszłość, tym bardziej opis
  w~niniejszej pracy będzie się różnił od stanu faktycznego.}.


\chapter{Alokator stron}

Ponieważ CMA integruje się dość mocno z~podsystemem zarządzania
pamięci (\ang{memory management}), do jej zrozumienia potrzeba
choć ogólnej wiedzy na temat tego w~jaki sposób Linuks zarządza
pamięcią \TODO{powtórzenie zarządza pamięcią}.

Linuks posiada wiele mechanizmów dzięki którym sterowniki mogą
alokować pamięć.  Począwszy od funkcji \lstinline|kmalloc()|
i \lstinline|vmalloc()|, poprzez mechanizmy puli pamięci, aż do
alokatora czasu bootowania i~alokatorów pamięci dostępnej dla urządzeń
zewnętrznych \cite[rozdział 8]{bib:ldd3}.  Pomimo, tak dużej liczby
interfejsów, wiele z~nich sprowadza się do alokatora stron (\ang{page
  allocator}), który jest sercem całego podsystemu zarządzania
pamięcią.

\section{Algorytm bliźniaków}

\begin{wrapfigure}{p}{0.4\textwidth}
\begin{center}
\includegraphics[width=0.30\textwidth]{build/alloc-free-cycle.eps}
\end{center}
\caption{Graficzna reprezentacja cyklu alokacji i~zwalniania buforów
  w~algorytmie bliźniaków.}
\end{wrapfigure}

Alokator stron implementuje algorytm bliźniaków (skąd też jego inna
angielska nazwa: {\it buddy system} lub {\it buddy allocator}), który
operuje na blakach o~rozmiarze $2^k$ jednostek.  W~przypadku Linuksa
jednostką jest pojedyncza strona fizyczna, a~na $k$~narzucone jest
ograniczenie $k < \mathrm{MAX\_ORDER}$.  \lstinline|MAX_ORDER| może
zależeć od architektury, na którą Linuks jest kompilowany, ale
zazwyczaj ma wartość $11$ (toteż na potrzeby tej pracy zakładam, iż $0
\le k \le 10$).

W~Linuksie, przez $k$ rozumie się rząd\TODO{może po prostu lepiej
  zostawić „order” i~nie tłumaczyć?} strony (\ang{page order}).
Strona rzędu 0 to pojedyncza strona fizyczna, strona rzędu 1
(\ang{1-order page}) to dwie strony fizyczne itd.\ aż do strony rzędu
10, czy też strony maksymalnego rzędu (\ang{max order page}), która
składa się z~1024 stron fizycznych.  Ogólnie, strona rzędu $n$ składa
się z~dwóch stron rzędu $n-1$.

Funkcja \lstinline|alloc_pages()|, która jest interfejsem dla
alokatora stron, przyjmuje jako argument właśnie rząd żądanej strony.

Z~powyższego opisu wynikają następujące właściwości alokatora stron.

\begin{itemize}
\item Nie można za jego pomocą zaalokować mniej niż jednej strony,
  tj.\ 4096 bajtów.
\item Interfejs zwyczajnie nie pozwala alokować obszarów, których
  rozmiar nie jest potęgą dwójki.
\item Gdyby jednak chcieć zaalokować taki obszar, wiązałoby się to
  z~potencjalnie dużą fragmentacją wewnętrzną.  Dla przykładu bufor
  dla kolorowej tekstury o~rozmiarze $512 \times 512$ pikseli
  wymagałby obszaru o~rozmiarze \unit[1]{MiB}, z~których
  \unit[256]{KiB}, a~więc \nicefrac{1}{4}, byłoby nieużywane.
\item Alokator stron nie jest w~stanie zaalokować obszaru większego
  niż \unit[4]{MiB}.  Z~tego powodu, nie nadaje się do alokowania
  ciągłego fizycznie buforu dla 5 megapikselowej kamery, czy nawet dla
  pojedynczej ramki {\it full HD}.
\end{itemize}

Jak zatem działa algorytm bliźniaków.  Alokator posiada listę wolnych
stron, których rząd jest pomiędzy $0$ a~$10$.  W~Linuksie zrealizowane
jest to poprzez 11 list dwukierunkowych, gdzie każda przeznaczona jest
dla stron o~konkretnym rzędzie.

Gdy sterownik chce zaalokować stronę rzędu $n$, alokator sprawdza
odpowiednią listę.  Jeżeli jest ona pusta, przechodzi do listy ze
stronami rzędu $n+1$, aż znajdzie wolną stronę (lub dojdzie do
maksymalnego rzędu, co sygnalizuje nieudaną alokacji.  Jeżeli uzyskana
w~ten sposób strona ma rząd większy niż żądany, jest ona dzielona na
pół, aż do osiągnięcia żądanego rozmiaru.  Strony, które powstały na
skutek podziału większej strony na pół, nazywamy stronami
bliźniaczymi.  Cały proces ilustruje algorytm \ref{alg:buddy-alloc}

\begin{algorithm}\label{alg:buddy-alloc}
\caption{Alokacja strony rzędu $k$ w~algorytmie bliźniaków}
\begin{algorithmic}[1]
\Require $0 \leq k < \mathrm{MAX\_ORDER}$
\Function{AllocatePage}{$k$}
    \State $i \gets k$
    \While {lista stron rzędu $i = \emptyset$}
        \State $i \gets i + 1$
        \If {$i = \mathrm{MAX\_ORDER}$}
            \State \Return $\emptyset$
        \EndIf
    \EndWhile

    \State $p \gets$ strona z listy stron rzędu $i$
    \While {$i \neq k$}
        \State $i \gets i - 1$
        \State podziel $p$ na pół na $p_1$ i $p_2$
        \Comment{Strony $p_1$ i $p_2$ nazywamy stronami bliźniaczymi}
        \State $p \gets p_1$
        \State dodaj $p_2$ do listy stron rzędu $i$
    \EndWhile
    \State \Return $p$
\EndFunction
\end{algorithmic}
\end{algorithm}

Przy zwalnianiu, dopóki to możliwe, strona jest łączona ze swoją
bliźniaczą stroną, dzięki czemu strony są dodawane do lista wolnych
stron o~dużym rzędzie.  Proces ten ilustruje algorytm
\ref{alg:buddy-free}

\begin{algorithm}\label{alg:buddy-free}
\caption{Zwalnianie strony $p$ rzędu $k$ w algorytmie bliźniaków}
\begin{algorithmic}[1]
\Procedure{FreePage}{$p$, $k$}
    \While {$k + 1 \neq \mathrm{MAX\_ORDER} \wedge p$ posiada wolną stronę bliźniaczą}
        \State $p' \gets$ strona bliźniacza $p$
        \State usuń $p'$ z~listy wolnych stron
        \State $k \gets k + 1$
        \State $p~\gets$ strona powstała w~wyniku połączenia $p$ i~$p'$
    \EndWhile
    \State dodaj $p$ do listy wolnych stron rzędu $k$\label{alg:buddy-free:add}
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Typy migracji}\label{sec:migratetype}

Powyższy opis pomija niektóre szczegóły alokatora stron.  Po pierwsze,
nawet jeżeli w~danej chwili nie istnieje wolna strona o~żądanym
rzędzie, aktywowana jest tzw.\ wolna ścieżka (\ang{slow path}), która
wykorzystuje różne mechanizmy odzyskiwania pamięci (np.\ poprzez
zwalnianie buforów dyskowych, czy w~najgorszym przypadku zabiciu
jednego z~działających procesów).  Mechanizmy te nie są jednak istotne
dla tematyki niniejszej pracy i~dlatego zostały pominięte.

Ważnym czynnikiem są typy migracji (\ang{migratetype}), których jest
sześć: {\it unmovable}, {\it reclaimable}, {\it movable}, {\it cma},
{\it reserve} oraz {\it isolate}).

\begin{itemize}
\item Dla potrzeb tej pracy traktuję typy {\it unmovable}, {\it
  reclaimable} i~{\it reserve} jak jeden typ -- typ nieruchomy.  To
  uproszczenie wynika z~faktu, iż dla CMA-y istotne jest tylko
  rozróżnienie pomiędzy stroną ruchomą i~nieruchomą.
\item Strony które są typu ruchomego charakteryzują się tym, że ich
  adres fizyczny nie jest istotny, w~związku z~czym mogą być
  przeniesione w~inne miejsce pamięci RAM.
\item Typ cma jest nowym typem dodanym przez CMA i~jest opisany
  dokładniej w~podrozdziale \ref{sec:migrate-cma}
\item Typ {\it isolate} jest niejako pseudo-typem, gdyż jeżeli wolna
  strona ma taki typ, nie może ona zostać zaalokowana.  Więcej na
  temat sposobu w~jaki ten typ może być wykorzystywany opisuję
  w~podrozdziale \ref{sec:alloc-contig-range}
\end{itemize}

Jednym z~przykładów stron ruchomych są strony anonimowe działających
procesów.  Ponieważ program odwołują się do nich poprzez mapowania
wirtualne, o~ile tablice translacji zostaną uaktualnione, zawartość
strony może być przeniesiona w~dowolne miejsce.  Podobnie wygląda
sprawa z~buforami dyskowymi i~wieloma innymi strukturami, którymi
zarządza jądro.

Proces przenoszenia ruchomej strony z~jednego miejsca w~inne nazywa
się migracją i wykorzystywany jest między innymi przy obsłudze
hot-swapu pamięci, a~także w~trackie procesu zagęszczania (opisanego
w~\cite{bib:compaction}), którego celem jest zwiększenie liczby
dostępnych stron o~wysokich rzędach.

Wołając funkcję \lstinline|alloc_pages()|, typ migracji strony jest
przekazywany jako argument, co pozwala alokatorowi stron grupować
strony tego samego typu.  Jest to istotne, gdyż mechanizm zagęszczania
nie działa zbyt dobrze jeżeli strony ruchomy przelatywały się
z~pozostałymi stronami, które nie podlegają migracji.

\section{Grupy stron}

Grupowanie to realizowane jest poprzez podział pamięci na bloki
składające się z~\lstinline|pageblock_nr_pages| stron (czy też
równoważnie na bloki rzędu \lstinline|pageblock_order|).  Konkretne
wartości tych stałych zależą od architektury, no którą jądro zostało
skompilowane, jak i~opcji konfiguracyjnych wybranych w~trakcie
kompilacji.  Niemniej, przeważnie wartość tych stałych to odpowiednio
1024 (stron) i~(rząd) 10 i~właśnie takie są przyjęte w~tej pracy.

Każdy blok stron (\ang{pageblock}) ma przypisany typ migracji,
a~alokator stron posiada oddzielne listy wolnych stron dla każdego
typu migracji.  Zatem patrząc na algorytm \ref{alg:buddy-alloc} należy
zdawać sobie sprawę, iż rozpatruje on listy wolnych stron danego typu
migracji.

\section{Zmiana typu migracji}\label{sec:type-change}

Należy pamiętać, iż dla jądra zrealizowanie alokacji jest ważniejsze
od trzymania stron o~tym samym typie migracji razem.  Dlatego dla
każdego typu migracji istnieje list zapasowych (\ang{fallback}) typów
migracji.  Jeżeli alokacja dla żądanego typu migracji nie powiedzie
się, alokator stron będzie próbował z~kolejnymi typami z~list, tak jak
to pokazuje algorytm \ref{alg:buddy-fallback}

Co więcej, jeżęli rząd żądanej strony jest dostatecznie duży, typ
migracji wszystkich wolnych stron w~danym bloku stron zostaje
zmieniany na ten zgodny z~wywołąniem funkcji
\lstinline|alloc_pages()|.

\begin{algorithm}\label{alg:buddy-fallback}
\caption{Alokacja strony rzędu $k$ z~uwzględnieniem typu migracji $m$}
\begin{algorithmic}[1]
\Function{ChangeBlockMigrateType}{$b$, $m$}
\State zmień typ migracji $b$ na $m$
\ForAll {wolnych stron $p' \in b$}
    \State przenieś $p'$ na listę wolnych stron typu $m$
\EndFor
\EndFunction
\Statex
\Function{AllocPageMigrateType}{$k$, $m$}
    \State $f \gets$ lista zapasowych typów migracji dla typu $m$
    \State dodaj $m$ na początek $f$
    \ForAll{$m' \in f$}
        \State $p \gets$ \Call{AllocPage}{$k$} biorąc pod uwagę listy stron typu $m'$
        \If {$p \neq \emptyset$}
            \If {$m \neq m' \wedge k \geq \nicefrac{\mathrm{page\_order}}{2}$}
                \State $b \gets$ blok stron zawierający $p$
                \State \Call{ChangeBlockMigrateType}{$b$, $m$}
            \EndIf
            \State \Return $p$
        \EndIf
    \EndFor
    \State \Return $\emptyset$
\EndFunction
\end{algorithmic}
\end{algorithm}

Podczas zwalniania, gdy strona jest dodawana do listy wolnych stron
(wideczne w~linii \ref{alg:buddy-free:add} algorytmu
\ref{alg:buddy-free}) typ migracji listy, na którą strona trafia
determinowany jest poprzez typ migracji przypisany blokowi stron do
którego dana strona należy.

Istotne jest tutaj, aby zauważyć, iż bloki stron mogą zmieniać swój
typ migracji, a~także, że nawet jeżeli blok ma dany typ migracji,
strony o~innym typie migracji mogą być z~niego przydzielone.


\section{Listy PCP}

Ostatnim istotnym, z~punktu widzenia CMA, aspektem alokatora stron są
listy PCP.  Ponieważ listy wolnych stron są współdzielone w~obrębie
całego systemu dostęp do nich musi być synchronizowany pomiędzy
wszystkimi procesorami.  Aby uniknąć kosztów związanych
z~synchronizacją, każdy procesor posiada swoje prywatne PCP listy, na
których znajdują się wolne strony rzędu 0.  Biorąc również i~ten
aspekt pod uwagę, alokacja przyjmuje postać przedstawianą w~algorytmie
\ref{alg:buddy-pcp}

\begin{algorithm}\label{alg:buddy-pcp}
\caption{Alokacja strony rzędu $k$ z~typem migracji $m$
  z~uwzględnieniem list PCP}
\begin{algorithmic}[1]
\Function{AllocPageUsePCP}{$k$, $m$}
    \If {$k \neq 0$}
        \State $p \gets$ \Call{AllocPageMigrateType}{$k$, $m$}
    \Else
        \State $l \gets$ lista PCP dla typu migracji $m$
        \If {$l = \emptyset$}
            \State $i \gets 0$
            \Repeat
                \State $p \gets$ \Call{AllocPageMigrateType}{$0$, $m$}
                \If {$p \neq \emptyset$}
                    \State dodaj $p$ do $l$
                    \State $i \gets i + 1$
                \EndIf
            \Until {$i \geq n \vee p = \emptyset$} \Comment{Wartość
              $n$ jest zależna od różnych czynników}
        \EndIf
        \If {$l = \emptyset$}
            \State \Return $\emptyset$
        \Else
            \State $p \gets$ pierwsza strona z $l$
            \State usuń pierwszą stronę z $l$
            \State lista PCP dla typu migracji $m$ $\gets l$
        \EndIf
    \EndIf
    \State \Return $p$
\EndFunction
\end{algorithmic}
\end{algorithm}


\chapter{Contiguous Memory Allocator}\label{sec:cma}

Podstawowym założeniem CMA jest umożliwienie alokowania dużych
obszarów ciągłych fizycznie bez konieczneści rezerwacji na wyłączność
dużej ilości pamięci.  Aby to umożliwić, CMA korzysta z~mechanizmu
migracji stron opisanego w~podrozdziale \ref{sec:migratetype}.

\section{Typ migracji CMA}\label{sec:migrate-cma}

Migracja jest możliwa tylko dla stron ruchomych.  Niestety, przed
zaimplementowaniem CMA, w~Linuksie nie istniał mechanizm, który
pozwalałby zagwarantować, aby w~systemie istniał duży obszar, w~którym
strony są wolne albo ruchome.  Ponieważ (jak opisałem w~podrozdziale
\ref{sec:type-change}) jądro dopuszcza alokacje nieruchomych stron
z~bloków stron oznaczonych jako przechowujące strony ruchome, a~także
posiada mechanizm na skutek którego bloki stron zmieniają swój typ,
oznaczenie bloku stron jako ruchome nie gwarantuje, że tylko strony
ruchome będą alokowane z~tego bloku stron.

Z~tego powodu, aby CMA dobrze działała, pierwszym krokiem było
stworzenie nowego typu migracji, nazwanym po prostu typem migracji CMA
(\lstinline|MIGRATE_CMA|).  Typ ten posiada dwie bardzo istotne cechy:

\begin{itemize}
\item Z~bloków stron oznaczonych typem CMA mogą być alokowane tylko
  strony ruchome.  Innymi słowy, typ migracji CMA istnieje w~lista
  zaposowych typów migracji jedynie dla typu ruchomego.
\item Blok oznaczony typem CMA nie zmienia swojego typu (na skutek
  działania alokatora stron).
\end{itemize}

O~ile pierwsza właściwość jest stosunkowo prosta do osiągnięcia,
zagwarantowania niezmienności typu bloku stron wymagało
zidentyfikowaniem wszystkich sytuacji, w~których blok może zmienić
swój typ (a~także sytuacji, w~których strony mogą trafić na listy
wolnych stron dla typu niezgodnego z~typem bloku, do którego należą)
i~dodanie odpowiednich warunków zapewniających, że nieporządana zmiana
nie nastąpi.

\section{Alokowanie obszaru pamięci}\label{sec:alloc-contig-range}

W~sytuacji, gdy istnieje gwarancja, że dany zakres stron posiada
jedynie strony wolne i~ruchome, można przystąpić do jej alokacji.
Drugim krokiem implementowania CMA było zatem stworzenie funkcji,
które dostaje jako argument zakres stron, a~następnie migruje
wszystkie zajęte strony, a~wolne usuwa z~listy wolnych stron.

\begin{algorithm}\label{alg:alloc-contig-range}
\caption{Alokacja strony z~podanego zakresu}
\begin{algorithmic}[1]
\Function{AllocContigRange}{$s$, $e$}
    \State $s' \gets s$ wyrównane w~dół do płnej strony maksymalnego rzędu
    \State $e' \gets e$ wyrównane w~górę do płnej strony maksymalnego rzędu
    \State izoluj strony z~zakresu $\left< s', e' \right)$
    \State migruj zajęte strony z~zakresu $\left< s, e~\right)$
    \State wyczyść PCP listy
    \ForAll{$p \in $\left< s', e' \right)$}
        \State usuń $p$ z listy wolnych stron
    \EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}
